%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\cleardoublepage
\chapter{Univariate distribution fitting}
\label{apx:A}
%*******************************************************************************

This appendix recalls the main methods to infer a univariate distribution considering a $n$-sized i.i.d. sample $X_n = \left\{x^{(1)}, \dots, x^{(n)}\right\} \in \R^{n}$. 
Inference techniques are split into two main groups, the parametric ones assuming that the underlying distribution belongs to a parametric distribution, and the nonparametric ones otherwise. 
In general, nonparametric methods require a large amount of data but allow more flexibility. 
In practice, nontrivial distributions (e.g., multimodal) might be easier to model using nonparametric approaches.

To assess the quality of this inference, a panel of goodness-of-fit methods are proposed \citep{saporta_2006}, this appendix recalls a few of them. 

%============================================================%
%============================================================%
\section*{Main parametric methods}
%============================================================%
%============================================================%

%============================================================%
\subsection*{Moments method}
%============================================================%
The moment's method looks for a parametric distribution with density $f_X(\btheta)$, whose first moments (e.g., $m(\btheta)$ and $\sigma^2(\btheta)$) match 
the empirical moments of the sample $X_n$ (e.g., $\what{m}_{X_n}$ and $\what{\sigma}^2$). After computing the empirical moments: 
\begin{equation}
    \what{m}_{n} = \frac1n \sum_{i=1}^{n} x^{(i)}, \qquad \what{\sigma}^2_n = \frac{1}{n-1} \sum_{i=1}^{n} \left(x^{(i)} - \what{m}_{X_n}\right)^2,
\end{equation} 
one can solve the system of equations $\left(m(\btheta) = \what{m}_{n}; \ \sigma^2(\btheta) = \what{\sigma}_n^2\right)$ to determine the optimal set of parameters $\btheta$ in this situation. 
Note that some families of distributions might be more suited to this method because of the analytical expression of their moments. 
However, this technique is sensitive to the possible biases in the estimation of the sample moments.

%============================================================%
\subsection*{Maximum likelihood estimation}
%============================================================%
Maximum likelihood estimation (MLE) is a popular alternative to the moments method. 
Similarly, maximizes a given correspondence metric between the dataset $X_n$ and a parametric distribution with density $f_X(\btheta)$.
This metric is the \textit{likelihood} function, defined as:
\begin{equation}
    \mathcal{L}(\btheta|X_n) = \prod_{i=1}^{n} f_X(x^{(i)}; \btheta), 
\end{equation}
with the PDF taking the set of parameters $\btheta$ written: $f_X(x^{(i)}; \btheta)$. 
For numerical reasons, the optimization is often performed on the natural logarithm of the likelihood function, called \textit{log-likelihood}. 
The goal is then finding the optimal vector $\what{\btheta}^*$ of parameters minimizing the following expression: 
\begin{equation}
    \what{\btheta}^* = \argmin_{\btheta \in \iD_{\btheta}} \left(-\sum_{i=1}^{n} \ln(f_X(x^{(i)}; \btheta)) \right).
\end{equation}

\fig{fig:MLE} illustrates the MLE by considering the fit of two distinct Weibull distributions w.r.t. a small set of observations $X_n = \{1, 2, 3, 4, 6\}$ (represented by the black bars on the x-axis).
Note that the quick analytical results from the moment method can be used as a starting point of the MLE optimization. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../numerical_experiments/chapter1/figures/MLE.png}
    \caption{Adequation of two different Weibull models using their likelihood with a sample of observations (black crosses).}
    \label{fig:MLE}
\end{figure}


%============================================================%
%============================================================%
\section*{Main nonparametric methods}
%============================================================%
%============================================================%


%============================================================%
\subsection*{Empirical CDF and histogram}
%============================================================%
The empirical CDF is a cumulative stair-shaped representation of the sorted sample $X_n$:
\begin{equation}
    \what{F}_X(x) = \frac1n \sum_{i=1}^{n} \1_{\left\{x^{(i)} \leq x\right\}}.
\end{equation}

A histogram consists of sorting and gathering the observations in a sample $X_n$ into a finite number of categories. 
These categories are called bins and each regroups the same number of observations (identical binwidth). 
The number of bins is the only tuning parameter of this method.

%============================================================%
\subsection*{Kernel density estimation}
%============================================================%
Kernel density estimation (KDE) is a nonparametric method, it estimates a PDF by weighing a sample of observations $X_n$ with kernels.  
After setting a kernel $k: \R \rightarrow \R_+$ and a scaling parameter $h>0$, also called bandwidth, the kernel density estimator is defined as:
\begin{equation}
    \what{f}_X(x) = \frac{1}{n h}\sum_{i=1}^{n} k\left(\frac{x - x^{(i)}}{h}\right).
\end{equation}

Different types of kernels are used for KDE, such as the standard normal, triangular, Epachnikov or uniform. 
The choice of bandwidth results in a bias-variance trade-off, that has been extensively discussed in the literature \citep{wand_jones_1994_kde}.
\fig{fig:KDE} illustrates the KDE for three different scaling parameters $h\in\{0.1, 0.2, 0.4\}$ applied on a set of observations (represented by the black bars on the x-axis). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../numerical_experiments/chapter1/figures/KDE.png}
    \caption{Fit of a bimodal density by KDE using different tuning parameters.}
    \label{fig:KDE}
\end{figure}


%============================================================%
%============================================================%
\section*{Main goodness-of-fit methods}
%============================================================%
%============================================================%

%============================================================%
\subsection*{Penalized likelihood criteria}
%============================================================%
Two quantitative goodness-of-fit criteria are commonly used to assess parametric inference: the \textit{Akaike information criterion} (AIC) and the \textit{Bayesian information criterion} (BIC). 
The likelihood as a goodness-of-fit criterion should only be applied to the same family of distributions. 
Otherwise, the comparison would unfairly advantage distributions with many degrees of freedom.
The two following criteria are metrics based on the likelihood with a correction related to the number of degrees of freedom of the distribution. 

The AIC and BIC are expressed as follows:
\begin{equation}
    \textrm{AIC} = \frac{-2 \ln\left(\mathcal{L}(\btheta|X_n)\right)}{n} + \frac{2 q}{n},\qquad
    \textrm{BIC} = \frac{-2 \ln\left(\mathcal{L}(\btheta|X_n)\right)}{n} + \frac{q \ln(n)}{n},
\end{equation}
with the likelihood $\mathcal{L}(\btheta|X_n)$ and the number of distribution's number degrees of freedom denoted $q$.
The second term adds a penalty depending on the number of parameters. 
The best inference will be given by the model with the smallest AIC or BIC. 
Note that an additional correction can be applied in a small data context.

%============================================================%
%\subsection*{Kolmogorov-Smirnov adequacy test}
%============================================================%



%============================================================%
\subsection*{Quantile-quantile plot}
%============================================================%

The quantile-quantile plot (also called QQ-plot) is a graphical tool providing a qualitative check of the goodness of fit.
It compares the CDF of the fitted model with the empirical CDF of the sample $X_n$.
To do so, it represents a scatterplot of the empirical quantiles (i.e., the ranked observations), against the quantiles of the fitted model at the levels 
$\left\{\alpha^{(i)}\right\}_{i=1}^n = \left\{\what{F}_X(x^{(i)})\right\}_{i=1}^n$.
The following \fig{fig:qqplot_kde} is a QQ-plot of the KDE model fitted in \fig{fig:KDE}. 
The closer the scatter plot gets to the first bisector line the better the fit is.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../numerical_experiments/chapter1/figures/qqplot.png}
    \caption{QQ-plot between the data from \fig{fig:KDE} and a KDE model.}
    \label{fig:qqplot_kde}
\end{figure}



