%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Sixth Chapter *****************************
%*******************************************************************************
\cleardoublepage
\chapter{Adaptive rare event estimation using Bernstein copula}
\label{chpt:6}
%*******************************************************************************
\hfill
\localtableofcontents
\newpage

%============================================================%
%============================================================%
\section{Introduction}
%============================================================%
%============================================================%

Assessing the reliability of systems' such as offshore wind turbines, often involves the estimation of rare event probabilities. 
In the reliability analysis framework introduced in Section~\ref{sec:reliability}, the performance of a system is typically modeled by a deterministic scalar function, denoted by $g: \iD_\bx \subseteq \R^d \rightarrow \R$, and referred to as the \textit{limit-state function}. 
A critical threshold on the system's output, denoted as $\yth \in \R$, then defines the \textit{failure domain}, expressed as $\iF_{\bx} := \{\bx \in \iD_\bx | g(\bx) \leq \yth\}$. 
Considering a probabilistic framework, the input uncertainties are modeled by a continuous random vector $\bX \in \iD_\bx$, with a joint probability density function (PDF) denoted by $f_\bX$. 
In this scenario, uncertainty propagation consists in composing the random vector $\bX$ with the function $g$ to obtain the output variable of interest $Y = g(\bX) \in \R$. 
Then, a common risk measure in reliability analysis is the \textit{failure probability}, denoted by $\pf$, representing the probability of the system exceeding the threshold $\yth$:
\begin{equation}
    \label{eq:failure_proba_c6}
    \pf := \P(g(\bX) \leq \yth)
        %&= \int_{\iF_\bx} f_\bX(\bx) \, \d\bx\\
        = \int_{\iD_\bx} \1_{\iF_\bx}(\bx) f_\bX(\bx) \, \dd\bx.
\end{equation}
%where $\1_{\iF_\bx}(\cdot)$ is the indicator function of the failure domain such that $\1_{\iF_\bx}(\bx) = 1$ if $\bx \in \iF_\bx$ and $\1_{\iF_\bx}(\bx) = 0$ otherwise. 

%Rare event problems are usually solved in the so-called \emph{standard normal space} after applying an ``iso-probabilistic transformation'' which can be either the Rosenblatt or the generalized Nataf one \citep{Lebrun_PHD_2013}.
%Additionally, the limit-state function $g$ can be viewed as an input-output ``black-box'' model which can be costly to evaluate (e.g., multi-physics wind turbine model), making the failure probability estimation nontrivial. 
%When the limit-state function is a costly computer model, one can build a surrogate model and use specific active learning methods (see, e.g., \citealt{moustapha_ss_2022}). 
%However, using surrogate models is not always possible for practical engineering applications as they might introduce another level of approximation, which can be prohibitive from safety auditing. 
%Moreover, their validation as well as their behavior with respect to large input dimension case make also their use quite complex (see, e.g.,  \citealt{marrel_iscream_2022}).

The main methods for rare event estimation (see Section~\ref{sec:reliability}) can be divided into two groups \citep{MorioBalesdent2015}: 
\textit{(i)} geometric approaches, such as the \emph{first-/second-order reliability method} (FORM/SORM) whose aim is to approximate the limit-state function by a first-/second-order Taylor expansion at the most probable failure point; 
\textit{(ii)} simulation-based techniques such as the \emph{crude Monte Carlo} method. 
Unfortunately, FORM/SORM methods do not provide a lot of statistical information as they are purely geometric approaches.
Meanwhile, estimating a rare event probability by crude Monte Carlo becomes rapidly intractable for engineering applications. 
To overcome this limit, advanced simulation techniques have been developed: among others, one can mention several ``variance reduction'' methods such as the non-adaptive and adaptive versions of the \emph{Importance Sampling} \citep{RubinsteinKroese1981} and splitting techniques \citep{cerou2012sequential} such as the \emph{Subset Simulation} (SS) \citet{AuBeck2001}.

In subset simulation, the idea is to write the rare event $\pf$ as a product of larger conditional probabilities, each one of them being easier to estimate. 
To generate intermediary conditional samples, this method uses Markov chain Monte Carlo (MCMC) sampling, which presents numerous versions \citep{Papaioannou_PEM_2015}. 
However, MCMC algorithms are known to be highly tunable algorithms which produce non-i.i.d. samples, which consequently, cannot always be used for direct statistical estimation (e.g., failure probability or sensitivity indices \citealp{daveiga_iooss_2021}). 

Adaptive importance sampling infers conditional distributions before using an importance sampling estimator of the failure probability. 
The set of auxiliary distributions converging towards the failure domain is either fitted by parametric approaches (e.g., using the cross-entropy method \citealt{kurtz_song_2013_aisce}), or nonparametric methods (e.g., using multivariate KDE \citealt{Morio_RESS_2011}). 
The main drawback of the cross-entropy adaptive importance sampling (AIS-CE) is its limited flexibility, not allowing it to perform well in the case of multimodal failure domains. 
On the other hand, nonparametric adaptive importance sampling (NAIS) inherits its drawbacks from the multivariate KDE (i.e., significant performance drop for medium to high dimension). 

% Importance sampling is a variance reduction method used to improve crude Monte Carlo, with various adaptations in the instrumental density construction. 
%Among others, one can refer to the ``adaptive importance sampling by cross-entropy'' \citep{kurtz_song_2013_aisce}, or the ``nonparametric adaptive importance sampling'' \citep{Morio_EJOP_2010}. 
%Alternatively, ``subset simulation'' (SS) \citep{AuBeck2001} (also referred to as ``multilevel splitting'' \citep{glasserman1999multilevel}, or ``sequential Monte Carlo'' \citep{cerou2012sequential}) is a different type of variance reduction method. 
%It constantly relies on the same idea: a rare event can be expressed as a product of nested conditional events (subsets). 
%The failure probability $\pf$ is written as a product of the respective conditional probabilities, each larger than $\pf$ (therefore easier to estimate). 
%To generate intermediary subset samples, this method uses Markov chain Monte Carlo (MCMC) sampling, which presents numerous versions \cite{Papaioannou_PEM_2015}.

The present work proposes a new rare event estimation method, ``Bernstein adaptive nonparametric conditional sampling'' (BANCS), adopting the same adaptive importance sampling structure as AIS-CE or NAIS while using a different mechanism to fit conditional distributions. 
This algorithm decomposes the fit of the intermediary conditional distributions into: a fit of their marginals by univariate KDE, and a fit of their copula with the \emph{Empirical Bernstein Copula} (introduced in Section~\ref{sec:nonparametric_copula}). 
Compared to direct multivariate KDE in NAIS, this decomposition significantly simplifies the inference in medium to high dimension. 
Additionally, the method proposed generates i.i.d. samples of the intermediary conditional distributions, unlike SS. 

In practice, such i.i.d. samples may also be used to estimate dedicated reliability-oriented sensitivity indices (see, e.g., \citealp{chabridon2021global,marrel_chabridon_2021}). 
The present chapter introduces kernel-based reliability-oriented sensitivity indices and their direct estimation as a post-processing of the BANCS algortihm. 

In this chapter, Section~\elias{xx} will introduce the BANCS algorithm for rare event estimation. 
Then, Section~\elias{xx} will apply this method to \elias{three} toy-cases and analyze the results with respect to NAIS and SS performances. 
Section~\elias{xx} will present a kernel-based reliability-oriented sensitivity index and illustrate their estimation on samples generated by the BANCS algorithm. 
Then, the last section present some conclusions and research perspectives.

%\elias{Terminology: elite set or elite sample can be used to describe the points that failed at each step}
\begin{remark}
    This contribution was initially presented in \citet{fekhari_ICASP_2023}, using the failure probability estimator proposed in the subset simulation algorithm (see \eq{eq:pf_SS}). 
    Switching to an adaptive importance sampling estimator (see e.g., \eq{eq:pf_NAIS}) notably improved the method.    
\end{remark}


%%============================================================%
%\subsection{Background}
%%============================================================%
%
%%------------------------------------------------------------%
%\subsubsection{subset simulation}
%%------------------------------------------------------------%
%
%subset simulation splits the failure event $\iF_{\bx}$ into an intersection of $k_\#$ intermediary events $\iF_{\bx} = \cap_{k=1}^{k_\#} \iF_{[k]}$.
%Each are nested such that $\iF_{[1]} \supset \dots \supset \iF_{[k_\#]} = \iF_{\bx}$.
%The failure probability is then expressed as a product of conditional probabilities:
%\begin{equation}
%    \pf = \P(\iF_{\bx}) = \P(\cap_{k=1}^{k_\#} \iF_{[k]}) = \prod_{k=1}^{k_\#} \P(\iF_{[k]} | \iF_{[k-1]}).
%\end{equation}
%From a practical point of view, the analyst tunes the algorithm by setting the intermediary probabilities $\P(\iF_{[k]} | \iF_{[k-1]}) = p_0, \forall k \in \{1, \dots, k_\# \}$. 
%Then, the corresponding quantiles $q_{[1]}^{p_0} > \dots > q_{[k_\#]}^{p_0}$ are estimated for each conditional subset samples $\bX_{[k], N}$ of size $N$. 
%Note that the initial quantile is estimated by crude Monte Carlo sampling on the input PDF $f_{\bX}$. 
%Following conditional subset samples are generated by MCMC sampling of $f_{\bX}(\bx |\iF_{[k-1]})$, using as seeds initialisation points the $n= N p_0$ samples given by $\mathbf{A}_{[k], n}=\{\bX_{[k-1]}^{(j)} \subset \bX_{[k-1], N}| g(\bX_{[k-1]}^{(j)}) > \widehat{q}_{[k-1]}^\alpha \}_{j=1}^n$. 
%This process is repeated until an intermediary quantile exceeds the threshold: $\widehat{q}_{[k_\#]}^{p_0} < \yth$. 
%Finally, the failure probability is estimated by:
%\begin{equation}
%    \pf \approx \widehat{\pf}^{\mathrm{SS}} = p_0^{k_\# -1} \frac1N \sum_{j=1}^N \1_{\{g(\bx) \leq \yth\}} (\bX_{[k_\#],N}^{(j)}).
%\end{equation}
%
%In practice, the subset sample size should be large enough to properly estimate intermediary quantiles, which leads \cite{AuBeck2001} to recommend setting $p_0=0.1$. 
%SS efficiency depends on the proper choice and tuning of the MCMC algorithm \citep{Papaioannou_PEM_2015}. 
%Our work uses the SS implementation from \texttt{OpenTURNS}\footnote{\href{https://openturns.github.io/www/index.html}{https://openturns.github.io/www/index.html}} \citep{baudin_dutfoy_2017} which integrates a component-wise Metropolis-Hastings algorithm. 
%As an alternative to generating samples on a conditional distribution by MCMC, one could try to fit this conditional distribution.
%
%%------------------------------------------------------------%
%\subsubsection{Multivariate modeling using copulas}
%%------------------------------------------------------------%
%The  Sklar theorem \citep{joe_1997} affirms that the multivariate distribution of any random vector $\mathbf{X} \in \R^d$ can be broken down into two objects:
%\begin{enumerate}
%    \item A set of univariate marginal distributions to describe the behavior of the individual variables;
%    \item A function describing the dependence structure between all variables, called a copula. 
%\end{enumerate}
%This theorem states that considering a random vector $\mathbf{X} \in \R^d$, with its distribution $F$ and its marginals $\{F_i\}_{i=1}^d$, there exists a copula $C: [0, 1]^d \rightarrow [0, 1]$, such that:
%\begin{equation}
%    F(x_1, \dots, x_d) = C\left(F_1(x_1), \dots, F_p(x_d)\right). 
%\end{equation}
%
%It allows us to divide the problem of fitting a joint distribution into two independent problems: fitting the marginals and fitting the copula. 
%Note that when the joint distribution is continuous, this copula is unique. 
%Provided a dataset, this framework allows to combine a parametric (or nonparametric) fit of marginals with a parametric (or nonparametric) fit of the copula. 
%When the distribution's dimension is higher than two, one can perform a parametric fit using vine copulas \citep{joe2011dependence}, implying the choice of multiple types of parametric copulas. 
%Otherwise, nonparametric fit by multivariate kernel density estimation (KDE) presents a computational burden as soon as the dimension increases \citep{chabridon2021global}. 
%Since univariate marginals are usually well-fitted with nonparametric tools (e.g., KDE), let us introduce an effective nonparametric method for copula fitting.


%============================================================%
%============================================================%
\section{Bernstein adaptive nonparametric conditional sampling}\label{sec:bancs}
%============================================================%
%============================================================%
This section presents a new method for rare event estimation named Bernstein adaptive nonparametric conditional sampling (BANCS) and illustrates its mechanism on a simple two-dimensional case. 

%============================================================%
%\subsection{BANCS algorithm}
%============================================================%

The BANCS algorithm uses the same structure as other adaptive importance sampling methods (e.g., NAIS or AIS-CE) while employing a different approach to fit the intermediate conditional distributions. 
As described in Algorithm \ref{alg:bancs}, at iteration $k$, after estimating the intermediary quantile $\widehat{q}_{[k]}^{p_0}$, a nonparametric model is fitted on the set $\mathbf{A}_{[k+1]}$ of all samples leading to values below $\widehat{q}_{[k]}^{p_0}$.  
This inference is done by coupling a set of marginals each fitted by KDE, with a copula fitted by EBC. 
The generation of the next i.i.d. $N$-sized sample $\bX_{[k+1], N}$ on the conditional distributions is straightforward and does not require MCMC sampling like in SS. 
Note that the BANCS method does not require iso-probabilistic transform.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
    \caption{Bernstein adaptive nonparametric conditional sampling (BANCS).}\label{alg:bancs}
    \footnotesize
    \begin{algorithmic}[1]
        \State \Comment{\textbf{Inputs:}}\hfill~
        \State $f_\bX$, joint PDF of the inputs
        \State $g(\cdot)$, limit-state function
        \State $\yth \in \R$, threshold defining the failure event 
        \State $N$, number of samples per iteration
        \State $m \in \N$, parameter of the EBC fitting
        \State $p_0 \in ]0, 1[$, empirical quantile order (rarity parameter)
        \State \Comment{\textbf{Algorithm:}}\hfill~
        \State Set $k = 0$ and $f_{[0]} = f_\bX$
        \State Sample $\bX_{[0], N} = \{\bx_{[0]}^{(i)}\}_{i=1}^N \overset{\text{i.i.d}}{\sim} f_{[0]}$
        \State Evaluate $Y_{[0], N} = \{g(\bx_{[0]}^{(i)})\}_{i=1}^N$
        \State Estimate the empirical $p_0$-quantile $\widehat{q}_{[0]}^{p_0}$ of the set $Y_{[0], N}$
        
        \While{$\widehat{q}_{[k]}^{p_0} > \yth$}
        \State Compute IS weights $\{w_{[k]}^{(i)}\}_{i=1}^N = \left\{\frac{f_\bX(\bx_{[k]}^{(i)})}{\widehat{f}_{[k]}(\bx_{[k]}^{(i)})}\right\}_{j=1}^N$
        \State Build a weighted elite-set $\mathbf{A}_{[k+1]}=\sum_{l=1}^{k} \sum_{i=1}^N w_{[k]}^{(j)} \, \1_{\{g(\bx_{[l]}^{(i)}) \leq \widehat{q}_{[k]} \}}(\bx_{[k]}^{(i)})$
        \State Fit marginals of the set $\mathbf{A}_{[k+1]}$ by KDE $\{\widehat{F}_j\}_{j=1}^d$
        \State Fit the copula of the set $\mathbf{A}_{[k+1]}$ by EBC $B_\bm(C_n)$
        \State Build a CDF $\widehat{F}_{[k+1]}(\bx) = B_m(C_n)(\widehat{F}_1(x_1), \dots, \widehat{F}_d(x_d))$
        \State Sample $\bX_{[k+1], N} = \{\bx_{[k+1]}^{(i)}\}_{i=1}^N \overset{\text{i.i.d}}{\sim} \widehat{f}_{[k+1]}$
        \State Evaluate $Y_{[k+1], N} = \{g(\bx_{[k+1]}^{(i)})\}_{i=1}^N$
        \State Estimate the empirical $p_0$-quantile $\widehat{q}_{[k+1]}^{p_0}$ of $Y_{[k+1], N}$
\State Set $k = k+1$
\EndWhile
\State Set $k_\# = k$
%\State Estimate $\widehat{\pf} = (1 - p_0)^{k_\#} \cdot \frac{1}{N} \sum_{j=1}^N \1_{\{g(\bX_{[k_\#]}^{(j)}) \geq \yth  \}}(\bX_{[k_\#]^{(j)}}) $
\State Estimate $\widehat{\pf}^{\mathrm{BANCS}} = \frac1N \sum_{i=1}^N w_{[k_\#]}^{(i)} \, \1_{\{g(\bx_{[k_\#]}^{(i)}) \leq \yth \}}(\bx_{[k_\#]}^{(i)})$
\State Estimate $\var(\widehat{\pf}^{\mathrm{BANCS}}) = \frac{1}{N-1} \left[\frac1N \sum_{i=1}^N (w_{[k_\#]}^{(i)})^2 \, \1_{\{g(\bx_{[k_\#]}^{(i)}) \leq \yth \}}(\bx_{[k_\#]}^{(i)}) \, - \, \left(\widehat{\pf}^{\mathrm{BANCS}}\right)^2\right]$
\State \Comment{\textbf{Outputs:}}\hfill~
\State $\widehat{\pf}^{\mathrm{BANCS}}$, estimate of $\pf$
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/bancs_illustration0.jpg}
        \caption{Iteration $k=0$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/bancs_illustration1.jpg}
        \caption{Iteration $k=1$.}
    \end{subfigure}
    \caption{BANCS algorithm applied to toy-case \#1: illustration of conditional sampling and nonparametric fit at the first and second iterations.}
    \label{fig:bancs_illustration1}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nonparametric inference implies tuning some parameters, either a scaling parameter for the KDE or a polynomial order $m$ (considered equal for all dimension) for the EBC. 
In BANCS algorithm, the KDE is tuned using Silverman's rule \elias{add ref} while the EBC is tuned according to the results minimizing the asymptotic mean integrated squared error (AMISE). 
As discussed in Chapter~\ref{chpt:3}, for a dataset with size $n$ and dimension $d$, the AMISE tuning for the EBC defined by \citet{sancetta_satchell_2004} as:
\begin{equation}\label{eq:amise_tuning}
    m_{\mathrm{AIMSE}} = 1 + n^{2/(d+4)}
\end{equation}
In our experience, EBC tuning in \eq{eq:amise_tuning} worked best for the BANCS algorithm and is systematically used in the following. 
This tuning sets a rather low polynomial order to the EBC, which avoids overfitting. 
For small sample sizes, e.g., $n<100$, \citet{segers_2017} showed the limits of this tuning. 
However, the typical sample sizes used for rare event simulation should be appropriate for the AMISE tuning. 

Ultimately, the estimator of the probability from \eq{eq:failure_proba_c6} is written as a simple IS estimator on the last conditional distribution with PDF $\widehat{f}_{[k_\#]}$:
\begin{equation}
    \widehat{\pf}^{\mathrm{BANCS}} = \frac1N \sum_{i=1}^N \frac{f_\bX(\bx_{[k_\#]}^{(i)})}{\widehat{f}_{[k_\#]}(\bx_{[k_\#]}^{(i)})} \, \1_{\{g(\bx_{[k_\#]}^{(i)}) \leq \yth \}}(\bx_{[k_\#]}^{(i)})\, , \quad \left\{\bx_{[k_\#]}^{(i)}\right\}_{i=1}^N  \overset{\text{i.i.d}}{\sim} \widehat{f}_{[k_\#]} \, .
\end{equation}
This estimator also benefits from an IS variance, which can also be estimated, using the same sample as previously, by:
\begin{equation}
    \var(\widehat{\pf}^{\mathrm{BANCS}}) = \frac{1}{N-1} \left[\frac1N \sum_{i=1}^N \left(\frac{f_\bX(\bx_{[k_\#]}^{(i)})}{\widehat{f}_{[k_\#]}(\bx_{[k_\#]}^{(i)})}\right)^2 \, \1_{\{g(\bx_{[k_\#]}^{(i)}) \leq \yth \}}(\bx_{[k_\#]}^{(i)}) \, - \, \left(\widehat{\pf}^{\mathrm{BANCS}}\right)^2\right]
\end{equation}

\fig{fig:bancs_illustration1} illustrates the nonparametric fit and conditional sampling in BANCS method on a two-dimensional reliability problem (later introduced as ``toy-case \#1''). 
At the iteration $k=0$, the conditional distribution fitted (with PDF represented by the blue isolines) slightly overlays below the quantile border $\widehat{q}_{[0]}^{p_0}$. 
Ideally, the conditional distribution fitted should be sharp around this border without overfitting the data
At the second iteration, the PDF of the second conditional distribution fitted is represented by the brown isolines. 
This second fit is realized on all the samples above the second quantile $\widehat{q}_{[1]}^{p_0}$, weighted by their respective distribution (see lines 14 and 15 from Algorithm~\ref{alg:bancs}). 
Including the samples from the previous iterations tends to make the fit sharper around the quantile border. 


%Tools to control the goodness of fit of nonparametric conditional distributions are also available. 
%As an example, let us consider the fitted conditional distribution at the first iteration (visible in \fig{fig:bancs_illustration1}). 
%Its quantile-quantile plot in \fig{fig:qqplot} shows a good fit of the two marginals by KDE. 
%Then, the goodness of fit of copulas can be evaluated by Kendall's plot, represented in \fig{fig:kendall_plot}. 
%This fit is also good, even if a slight bias is again visible.

%\begin{figure}[!ht]
%    \centering
%    \includegraphics[width=0.6\linewidth]{part3/figures/BANCS/qqplots.pdf}
%    \caption{QQ-plot for KDE of marginals of the conditional distribution from \fig{fig:bancs_illustration1}.}
%    \label{fig:qqplot}
%\end{figure}
%
%\begin{figure}[!ht]
%    \centering
%    \includegraphics[width=0.6\linewidth]{part3/figures/BANCS/kendall_plot.pdf}
%    \caption{Kendall plot for EBC on the copula of a conditional distribution from \fig{fig:bancs_illustration1}.}
%    \label{fig:kendall_plot}
%\end{figure}



%============================================================%
%============================================================%
\section{Numerical experiments}
%============================================================%
%============================================================%

In the present section, the performances of BANCS algorithm are compared with the ones from subset simulation (SS) and the nonparametric adaptive importance sampling (NAIS). 
SS efficiency depends on the choice and tuning of the MCMC algorithm \citep{Papaioannou_PEM_2015}. 
Our work uses the \texttt{OpenTURNS} implementations of the SS\footnotemark (integrating a component-wise Metropolis-Hastings algorithm), 
\footnotetext{SS: \url{https://openturns.github.io/openturns/latest/user_manual/_generated/openturns.SubsetSampling.html}} 
and the \texttt{OpenTURNS} implementations of NAIS\footnotemark. 
\footnotetext{NAIS: \url{https://openturns.github.io/openturns/latest/user_manual/_generated/openturns.NAIS.html}} 
An implementation of the BANCS method and the following numerical experiments are available in a Git repository\footnote{BANCS: \url{https://github.com/efekhari27/bancs}}. 

In the following analytical numerical experiments, the intermediary probabilities were set to $p_0=0.1$ (following the recommendations from \citet{AuBeck2001}), allowing a fair comparison with subset simulation. 
The size $N$ of each intermediate samples (i.e., subset) evolves in the following set $N\in \{300, 500, 700, 1000, 2000, 5000, 10 000\}$, in order to get a reasonable number of points to perform the nonparametric fitting. 
Let us reming that the EBC tuning is setup to minimize the AMISE, such that $m = 1 + n^{\frac{2}{d+4}}$. 
In order to take into account the variability of the method's results, each experiment is repeated 100 times, allowing the computation of a coefficient of variation $\widehat{\delta} = \frac{\sigma_{\widehat{\pf}}}{\mu_{\widehat{\pf}}}$. 

%============================================================%
\subsection{Analytical toy-cases}
%============================================================%
The reference values of the failure probabilities of each problem studied hereafter are obtained by Monte Carlo estimation on very large samples. 

%------------------------------------------------------------%
\paragraph{Toy-case \#1: Parabolic reliability problem.}
%------------------------------------------------------------%

Let us define the parabolic reliability problem, considering the function $g_1: \R^2 \rightarrow \R$:
\begin{equation}
    g_1(\bx)= (x_1 - x_2) ^ 2 - 8 (x_1 + x_2 - 5),
\end{equation}
with the input random vector $\bX = (X_1, X_2)$ following a standard 2-dimensional normal distribution. 
The reliability problem consists in evaluating: $p_{\mathrm{f}, 1} = \mathbb{P} ( g_1(\bX) \leq 0 ) = 1.31 \times 10^{-4}$.

%------------------------------------------------------------%
\paragraph{Toy-case \#2: Four-branch reliability problem.}
%------------------------------------------------------------%

Let us define the four-branch reliability problem (originally proposed by \cite{waarts2000structural}), considering the following function $g_2: \R^2 \rightarrow \R$:
\begin{align}
  g_2(\bx) = \min \begin{pmatrix}
    3+0.1(x_1-x_2)^2-\frac{(x_1+x_2)}{\sqrt{2}}\\
    3+0.1(x_1-x_2)^2+\frac{(x_1+x_2)}{\sqrt{2}}\\
    (x_1-x_2)+ \frac{7}{\sqrt{2}}\\
    (x_2-x_1)+ \frac{7}{\sqrt{2}}
  \end{pmatrix},
\end{align}
with the input random vector $\bX = (X_1, X_2)$ following a standard 2-dimensional normal distribution. 
The reliability problem consists in evaluating: $p_{\mathrm{f}, 2} = \mathbb{P} ( g_2(\bX) \leq 0 ) =  2.22 \times 10^{-3}$.

%------------------------------------------------------------%
\paragraph{Toy-case \#3: Medium-dimensional reliability problem.}
%------------------------------------------------------------%

Let us define the medium-dimensional reliability problem (proposed by \cite{yun2018efficient}), considering the following function $g_3 : \R^7 \rightarrow \R$:

\begin{equation}
    g_3(\bx) = 15.59 \times 10^4 - \frac{x_1 x_3^2}{2 x_3^2} \frac{x_2^4 - 4 x_5 x_6 x_7^2 + x_4 (x_6 + 4 x_5 + 2 x_6 x_7)}{x_4 x_5 (x_4 + x_6 + 2 x_6 x_7)},
\end{equation}
with the input random vector $\bX = (X_1, \dots, X_7)$, following a product of normal distributions defined in \cite{yun2018efficient}. 
The reliability problem consists in evaluating: $p_{\mathrm{f}, 3} = \mathbb{P} ( g_3(\bX) \leq 0 ) =  8.10 \times 10^{-3}$.


%------------------------------------------------------------%
\paragraph{Toy-case \#4: Medium-dimensional oscillator problem.}
%------------------------------------------------------------%
Let us define the higher-dimensional oscillator reliability problem (adapted from \elias{add ref}), considering the following function $g_4 : \R^8 \rightarrow \R$:

\begin{equation}
    g_4(\bx) = F_s - 3 k_s \sqrt{\frac{\pi S_0}{4 \zeta_s \omega_s^3} \, \frac{\zeta_a \zeta_s}{\zeta_p \zeta_s (4 \zeta_a^2 + \theta^2) + \gamma \zeta_a^2} \, \frac{\omega_p \, (\zeta_p \omega_p^3 + \zeta_s \omega_s^3)}{4 \zeta_a \omega_a^4}},
\end{equation}
where $\bx = \left(m_p, m_s, k_p, k_s, \zeta_p, \zeta_s, F_s, S_0\right)$,  
$\quad \omega_p=\sqrt{k_p/m_p}, \quad \omega_s=\sqrt{k_s/m_s}, \quad \omega_a = (\omega_p + \omega_s)/2, \quad $
$\zeta_a = (\zeta_p + \zeta_s)/2, \quad \gamma = m_s/m_p, \quad \theta= (\omega_p-\omega_s)/\omega_a$. 
The input random vector $\bX$, defined as a product of marginals following the distributions defined in Table~\ref{tab:oscillator}. 
The reliability problem consists in evaluating: $p_{\mathrm{f}, 4} = \mathbb{P} ( g_4(\bX) \leq 0 ) =  3.78 \times 10^{-7}$.

\begin{table}[h]
\centering
\begin{tabular}{ lcccccccc }
    \hline
    Variable            & $m_p$ & $m_s$ & $k_p$ & $k_s$ & $\zeta_p$ & $\zeta_s$ & $F_s$ & $S_0$ \\
    \hline          
    Distribution        &  \multicolumn{8}{c}{Lognormal} \\ 
    Mean                & 1.5 & 0.01 & 1.0 & 0.01 & 0.05 & 0.02 & 27.5 & 100.0\\ 
    Coeff. of variation & 0.1 & 0.1 & 0.2 & 0.2 & 0.4 & 0.5 & 0.1 & 0.1\\
    \hline
\end{tabular}
\caption{Random inputs from the toy-case \#4.}
\label{tab:oscillator}
\end{table}

%============================================================%
\subsection{Results analysis}
%============================================================%

\begin{itemize}
    \item do
\end{itemize}

Results of our numerical experiments are presented graphically (for 2-dimensional problems) in \fig{fig:2D_toycase_reliability}, and numerically in Table \ref{tab:result_table}. 


In the same fashion as the previous illustrations, the figures represent the intermediary quantiles $\widehat{q}_{[k]}^{p_0}$ estimated over conditional samples of size $N=10^4$. 
Moreover, samples $\mathbf{A}_{[k+1], n}$ exceeding these quantiles are also represented in the same color. 
Notice how the last estimated quantile is set to the problem threshold $\yth = 0$. 
To capture the dispersion of BANCS estimation, 100 repetitions were realized. \elias{Since the vairance provided by the SS is only approximated on bootstraping repetitions seems more fair.}
Let us notice that for each toy-case, BANCS well estimates the failure probabilities' orders of magnitude. 
Yet the numerical values in Table \ref{tab:result_table} consistently present a positive bias, leading to an overestimated failure probability. 
This bias is partially explained by the EBC tuning choice and could be reduced at the expense of a slightly higher variance.

The variance obtained with the repetitions is quite large. 
Although, part of it is due to the fact that the algorithm might compute a different total number of subsets (e.g., toy-case \#1 is either solved in four or five subsets). 
Overall, considering the EBC tuning from \eq{eq:kimse}, BANCS performs worst than SS on toy-cases \#1 and \#2 but performs as well as SS on the toy-case \#3. 
This might be due to the fact that toy-case \#3 has a higher input dimension. 
However, one can note that SS coefficient of variation is computed by an approximation, tending to underestimate the true coefficient of variation (see e.g., \cite{Papaioannou_PEM_2015}). 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/bancs_parabolic.jpg}
        \caption{Toy-case \#1.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/bancs_4branch.jpg}
        \caption{Toy-case \#2.}
    \end{subfigure}
    \caption{BANCS iterations on the two-dimensional reliability problems (for $N=10^4$ and $p_0=0.1$). 
    Only the samples exceeding the intermediary thresholds are represented.}
    \label{fig:2D_toycase_reliability}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/RP4B_mean.png}
        \caption{Failure probability (toy-case \#2).}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/RP4B_cov.png}
        \caption{Coefficient of variation (toy-case \#2).}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/RP38_mean.png}
        \caption{Failure probability (toy-case \#3).}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/RP38_cov.png}
        \caption{Coefficient of variation (toy-case \#3).}
    \end{subfigure}

    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/Oscillator_mean.png}
        \caption{Failure probability (toy-case \#3).}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{part3/figures/BANCS/Oscillator_cov.png}
        \caption{Coefficient of variation (toy-case \#3).}
    \end{subfigure}


    \caption{Todo.}
    \label{fig:bancs_results}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
    \centering
    \caption{Results of the numerical experiments (subset sample size $N=10^4$, $p_0=0.1$).}
    \begin{tabular}{|l|l|l||l|l||l|l|}\hline
     & $d$ & $\pf^{\mathrm{ref}}$ & $\widehat{\pf}^{\mathrm{BANCS}}$ & $\widehat{\delta}^{\mathrm{BANCS}}$ & $\widehat{\pf}^{\mathrm{SS}}$ & $\widehat{\delta}^{\mathrm{SS}}$\\
    \hline
    Toy-case \#1 & 2 & $1.31 \times 10^{-4}$ & $2.67 \times 10^{-4}$ & $24 \%$ & $1.30 \times 10^{-4}$ & $9 \%$\\
    \hline
    Toy-case \#2 & 2 & $2.21 \times 10^{-4}$ & $4.23 \times 10^{-4}$ & $7 \%$ & $2.24 \times 10^{-4}$ & $6 \%$\\
    \hline
    Toy-case \#3 & 7 & $8.10 \times 10^{-3}$ & $9.32 \times 10^{-3}$ & $15 \%$ & $8.92 \times 10^{-3}$ & $6 \%$\\ \hline
    \end{tabular}
    \label{tab:result_table}
\end{table*}



%============================================================%
%============================================================%
\section{Sequential reliability-oriented sensitivity analysis}
%============================================================%
%============================================================%
\begin{itemize}
    \item Definition of the formalism and the main methods (FORM alpha-factors, Sobol', T-HSIC, Shapley)
    \item Motivate the need
    \item Literature review 
    \item Definition of the T-HSIC 
    \item Post-processing BANCS results with dynamic T-HSIC (how does the ROSA evolve with the rarity?)
\end{itemize}





%============================================================%
%============================================================%
\section{Conclusion}
%============================================================%
%============================================================%
\begin{itemize}
    \item Learn from a weighted sample
    \item Compare the learning-time with NAIS
    \item Optimze the EBC tunning with a test set (possibly build by KH)
    \item What if the sampling is not iid?
    \item Advantage of working directly in the physical space directly
    \item Perspective learning the entire distribution with Bernstein polynomials
    \item We could fit the marginals with parametric models and compare
    \item EBC by blocs in high dimension 
\end{itemize}


Subset Simulation uses MCMC sampling to generate its intermediary conditional samples. 
However, MCMC algorithms tends to be complex to tune and does not generate i.i.d. conditional samples. 
In this work, a new method is proposed, replacing MCMC sampling with a simpler procedure. 
An intermediary conditional distribution is first fitted by a nonparametric approach, mixing kernel density estimation for fitting the marginals and Empirical Bernstein Copula (EBC) for fitting the copula. 
Then, the resulting allows to perform direct Monte Carlo sampling. 
This method is named ``Bernstein adaptive nonparametric conditional sampling'' (BANCS) and is applied to three toy-cases (two 2-dimensional and one 7-dimensional) and compared with SS.

The method shows promising results, even though a small positive bias consistently appears. 
This issue results from EBC tuning, creating a bias-variance tradeoff in the copula fit. 
Theoretical works offer optimal tuning, allowing us to find the optimal compromise. 
In our numerical experiments, an empirical estimation of BANCS variance is computed over a set of repetitions. 
BANCS estimated coefficient of variation is higher than SS approximated coefficient of variation. 
This work can be further explored by building an approximation of BANCS variance and confidence interval. 
One major advantage remains that the samples generated at each iteration are i.i.d. leading to a possible use of these samples to perform global reliability-oriented sensitivity analysis \citep{marrel_chabridon_2021} 
in order to detect and analyze the most influential input variables leading to failure.



%\section{Application to wind turbine fatigue reliability}
%\section{Application to a floating offshore wind turbine reliability}